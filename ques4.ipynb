{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ab9b75",
   "metadata": {},
   "source": [
    "# Answer4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db4ed8",
   "metadata": {},
   "source": [
    "## District-peaks.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36babf",
   "metadata": {},
   "source": [
    "### Calculation of peak weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bf42488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#dictionary to replace district names with disrict keys in district.csv\n",
    "dist_code_for_q3=pd.read_csv('datasets/dcodeq3.csv')\n",
    "dist_code_for_q3=dist_code_for_q3['B'].to_list()\n",
    "dist_name_for_q3=pd.read_csv('datasets/dnameq3.csv')\n",
    "dist_name_for_q3=dist_name_for_q3['A'].to_list()\n",
    "for i in range(len(dist_name_for_q3)):\n",
    "    dist_name_for_q3[i]=dist_name_for_q3[i].lower()\n",
    "dict_for_q3=dict(zip(dist_name_for_q3,dist_code_for_q3))\n",
    "\n",
    "\n",
    "\n",
    "district_df=pd.read_csv('datasets/district-df.csv')   #district-df.csv was created in ques3\n",
    "\n",
    "# week of thursday to wednesday\n",
    "dist_keyq4_list=[]\n",
    "weekq4_list=[]\n",
    "casesq4_list=[]\n",
    "cumulativeq4_dict=dict(zip(dist_code_for_q3,[0]*len(dist_code_for_q3)))\n",
    "counter_q4=1\n",
    "cases_q4=0\n",
    "week_q4=2\n",
    "n1_q4=len(district_df)\n",
    "i=0\n",
    "while '30' not in district_df['Date'].iloc[i]:\n",
    "    i=i+1\n",
    "i=i+1\n",
    "while i<n1_q4:\n",
    "    \n",
    "    \n",
    "    if district_df['Date'].iloc[i]==district_df['Date'].iloc[i-1]:\n",
    "        i=i+1\n",
    "        continue\n",
    "      \n",
    "    counter_q4=counter_q4+1      #for keeping count of days \n",
    "    \n",
    "    if counter_q4==7:       #last day of week\n",
    "        \n",
    "        j=0\n",
    "        while i+j<n1_q4-1:\n",
    "            \n",
    "            cases_q4=district_df['Confirmed'].iloc[i+j]\n",
    "            temp=cases_q4\n",
    "            cases_q4=cases_q4-cumulativeq4_dict[district_df['District'].iloc[i+j]]\n",
    "            dist_keyq4_list.append(district_df['District'].iloc[i+j])\n",
    "            weekq4_list.append(week_q4)\n",
    "            casesq4_list.append(cases_q4)\n",
    "            cumulativeq4_dict[district_df['District'].iloc[i+j]]=temp\n",
    "            if district_df['Date'].iloc[i+j] != district_df['Date'].iloc[i+j+1]:\n",
    "                break\n",
    "            j=j+1\n",
    "            \n",
    "        \n",
    "        counter_q4=0\n",
    "        week_q4=week_q4+2\n",
    "        \n",
    "    i=i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a005546",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'District':dist_keyq4_list,'week':weekq4_list,'cases':casesq4_list}\n",
    "type2_week_df=pd.DataFrame(data)\n",
    "#type2_week_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a232f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#week of sunday to saturday\n",
    "#calculation will be done by importing cases-week.csv and changing weeks from 1,2,3,4... to 1,3,5,7....\n",
    "type1_week_df=pd.read_csv('output files/cases-week.csv')\n",
    "week_q4_list=type1_week_df['week'].to_list()\n",
    "n2_q4=len(type1_week_df)\n",
    "i=0\n",
    "while i<n2_q4:\n",
    "    week_q4_list[i]=2*week_q4_list[i] - 1\n",
    "    i=i+1\n",
    "    \n",
    "type1_week_df['week']=week_q4_list\n",
    "#type1_week_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e5b8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapped_week_df=pd.merge(type1_week_df,type2_week_df,how='outer')\n",
    "overlapped_week_df=overlapped_week_df.sort_values('week')\n",
    "#overlapped_week_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f17251b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cases_dict=dict(zip(dist_code_for_q3,[0]*len(dist_code_for_q3)))\n",
    "max_wave1_weekid_dict=dict(zip(dist_code_for_q3,[1]*len(dist_code_for_q3)))\n",
    "for i in range(40000):\n",
    "    if overlapped_week_df['cases'].iloc[i] > max_cases_dict[overlapped_week_df['District'].iloc[i]]:\n",
    "        max_cases_dict[overlapped_week_df['District'].iloc[i]]=overlapped_week_df['cases'].iloc[i]\n",
    "        max_wave1_weekid_dict[overlapped_week_df['District'].iloc[i]]=overlapped_week_df['week'].iloc[i]\n",
    "       \n",
    "    \n",
    "test_df=pd.DataFrame.from_dict(max_wave1_weekid_dict.items())\n",
    "test1_df=pd.DataFrame()\n",
    "test1_df['District']=test_df[0]\n",
    "test1_df['wave1-weekid']=test_df[1]\n",
    "#test1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7136107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cases_dict=dict(zip(dist_code_for_q3,[0]*len(dist_code_for_q3)))\n",
    "max_wave2_weekid_dict=dict(zip(dist_code_for_q3,[1]*len(dist_code_for_q3)))\n",
    "for i in range(40001,83000):\n",
    "    if overlapped_week_df['cases'].iloc[i] > max_cases_dict[overlapped_week_df['District'].iloc[i]]:\n",
    "        max_cases_dict[overlapped_week_df['District'].iloc[i]]=overlapped_week_df['cases'].iloc[i]\n",
    "        max_wave2_weekid_dict[overlapped_week_df['District'].iloc[i]]=overlapped_week_df['week'].iloc[i]\n",
    "        \n",
    "test_df=pd.DataFrame.from_dict(max_wave2_weekid_dict.items())\n",
    "test2_df=pd.DataFrame()\n",
    "test2_df['District']=test_df[0]\n",
    "test2_df['wave2-weekid']=test_df[1]\n",
    "#test2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef3ce0",
   "metadata": {},
   "source": [
    "### Calculation of peak months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "403fba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAVE-1\n",
    "monthq4_df=pd.read_csv('output files/cases-month.csv')\n",
    "monthq4_df=monthq4_df.sort_values('month')\n",
    "\n",
    "max_cases_dict=dict(zip(dist_code_for_q3,[0]*len(dist_code_for_q3)))\n",
    "max_wave1_monthid_dict=dict(zip(dist_code_for_q3,[1]*len(dist_code_for_q3)))\n",
    "for i in range(4300):\n",
    "    if monthq4_df['cases'].iloc[i] > max_cases_dict[monthq4_df['District'].iloc[i]]:\n",
    "        max_cases_dict[monthq4_df['District'].iloc[i]]=monthq4_df['cases'].iloc[i]\n",
    "        max_wave1_monthid_dict[monthq4_df['District'].iloc[i]]=monthq4_df['month'].iloc[i]\n",
    "        \n",
    "test_df=pd.DataFrame.from_dict(max_wave1_monthid_dict.items())\n",
    "test3_df=pd.DataFrame()\n",
    "test3_df['District']=test_df[0]\n",
    "test3_df['wave1-monthid']=test_df[1]\n",
    "#test3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9763160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#WAVE-2\n",
    "max_cases_dict=dict(zip(dist_code_for_q3,[0]*len(dist_code_for_q3)))\n",
    "max_wave2_monthid_dict=dict(zip(dist_code_for_q3,[1]*len(dist_code_for_q3)))\n",
    "for i in range(4301,9200):\n",
    "    if monthq4_df['cases'].iloc[i] > max_cases_dict[monthq4_df['District'].iloc[i]]:\n",
    "        max_cases_dict[monthq4_df['District'].iloc[i]]=monthq4_df['cases'].iloc[i]\n",
    "        max_wave2_monthid_dict[monthq4_df['District'].iloc[i]]=monthq4_df['month'].iloc[i]\n",
    "        \n",
    "test_df=pd.DataFrame.from_dict(max_wave2_monthid_dict.items())\n",
    "test4_df=pd.DataFrame()\n",
    "test4_df['District']=test_df[0]\n",
    "test4_df['wave2-monthid']=test_df[1]\n",
    "#test4_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acaa772",
   "metadata": {},
   "source": [
    "### creation of file districts-peak.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75f332dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_dist_list=pd.read_csv('datasets/q1-dist-list-df.csv')\n",
    "district_peak_df=pd.merge(test1_df,test2_df)\n",
    "district_peak_df=pd.merge(district_peak_df,test3_df)\n",
    "district_peak_df=pd.merge(district_peak_df,test4_df)\n",
    "district_peak_df=pd.merge(district_peak_df,q1_dist_list)   # removing all districts not in ques1 district list\n",
    "\n",
    "'''Some districts have all the entries as 1 of column wave1-weekid,  wave2-weekid, wave1-monthid and wave2-monthid\n",
    "This is supposedly due to reasons such as NEGATIVE ENTRIES in cowin-vaccine data and incorrect cummulative entries.\n",
    "These can be left as it is but I am going to remove them because they behave somewhat like outliers/noise'''\n",
    "#removing rows having all entries as 1\n",
    "district_peak_df=district_peak_df.drop(district_peak_df[(district_peak_df['wave1-weekid']==1)&(district_peak_df['wave2-weekid']==1)&(district_peak_df['wave1-monthid']==1)&(district_peak_df['wave2-monthid']==1) ].index,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "district_peak_df.to_csv('output files/district-peaks.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b977f1",
   "metadata": {},
   "source": [
    "## State-peaks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf1f03",
   "metadata": {},
   "source": [
    "### week-wise peak calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "040271a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting district data into state-wise data\n",
    "overlapped_week_df.to_csv('datasets/overlapped_week-df.csv',index=False)\n",
    "state_wise_df=pd.read_csv('datasets/overlapped_week-df.csv')\n",
    "state_wise_df=state_wise_df.sort_values('week')\n",
    "state_wise_df=state_wise_df.replace(to_replace='_.*',value='',regex=True)\n",
    "state_wise_df=state_wise_df.rename(columns={'District':'state'})\n",
    "\n",
    "state_list=state_wise_df['state'].unique().tolist()\n",
    "state_wise_df=state_wise_df.groupby(['state','week']).sum()  # here groupby is applied on state and week together. So we cannot\n",
    "                                                             # apply operation on state column alone. That is why i am saving csv file in next step\n",
    "\n",
    "\n",
    "state_wise_df=state_wise_df.sort_values('week')\n",
    "state_wise_df.to_csv('datasets/state-wise-df.csv')\n",
    "state_wise_df=pd.read_csv('datasets/state-wise-df.csv')\n",
    "\n",
    "#wave1\n",
    "max_cases_state_dict=dict(zip(state_list,[0]*len(state_list)))\n",
    "max_wave1_weekid_state_dict=dict(zip(state_list,[1]*len(state_list)))\n",
    "for i in range(2000):       # len(state_wise_df)=4156  , so i have traversed from row 1 to 1000 for wave-1\n",
    "    if state_wise_df['cases'].iloc[i] > max_cases_state_dict[state_wise_df['state'].iloc[i]]:\n",
    "        max_cases_state_dict[state_wise_df['state'].iloc[i]]=state_wise_df['cases'].iloc[i]\n",
    "        max_wave1_weekid_state_dict[state_wise_df['state'].iloc[i]]=state_wise_df['week'].iloc[i]\n",
    "        \n",
    "#wave2\n",
    "max_cases_state_dict=dict(zip(state_list,[0]*len(state_list)))\n",
    "max_wave2_weekid_state_dict=dict(zip(state_list,[1]*len(state_list)))\n",
    "for i in range(2001,4100):\n",
    "    if state_wise_df['cases'].iloc[i] > max_cases_state_dict[state_wise_df['state'].iloc[i]]:\n",
    "        max_cases_state_dict[state_wise_df['state'].iloc[i]]=state_wise_df['cases'].iloc[i]\n",
    "        max_wave2_weekid_state_dict[state_wise_df['state'].iloc[i]]=state_wise_df['week'].iloc[i]\n",
    "        \n",
    "#converting dictionaries to dataframe so that we can merge them at end easily\n",
    "test_df=pd.DataFrame.from_dict(max_wave1_weekid_state_dict.items())\n",
    "test5_df=pd.DataFrame()\n",
    "test5_df['state']=test_df[0]\n",
    "test5_df['wave1-weekid']=test_df[1]\n",
    "\n",
    "test_df=pd.DataFrame.from_dict(max_wave2_weekid_state_dict.items())\n",
    "test6_df=pd.DataFrame()\n",
    "test6_df['state']=test_df[0]\n",
    "test6_df['wave2-weekid']=test_df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545741c",
   "metadata": {},
   "source": [
    "### month-wise peak calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e33d1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting district data into state-wise data\n",
    "state_wise_month_df=pd.read_csv('output files/cases-month.csv')\n",
    "state_wise_month_df=state_wise_month_df.sort_values('month')\n",
    "state_wise_month_df=state_wise_month_df.replace(to_replace='_.*',value='',regex=True)\n",
    "state_wise_month_df=state_wise_month_df.rename(columns={'District':'state'})\n",
    "\n",
    "state_wise_month_df=state_wise_month_df.groupby(['state','month']).sum()  \n",
    "\n",
    "state_wise_month_df=state_wise_month_df.sort_values('month')\n",
    "state_wise_month_df.to_csv('datasets/state-wise-month-df.csv')\n",
    "state_wise_month_df=pd.read_csv('datasets/state-wise-month-df.csv')\n",
    "\n",
    "#wave1\n",
    "max_cases_state_dict=dict(zip(state_list,[0]*len(state_list)))\n",
    "max_wave1_monthid_state_dict=dict(zip(state_list,[1]*len(state_list)))\n",
    "for i in range(230):\n",
    "    if state_wise_month_df['cases'].iloc[i] > max_cases_state_dict[state_wise_month_df['state'].iloc[i]]:\n",
    "        max_cases_state_dict[state_wise_month_df['state'].iloc[i]]=state_wise_month_df['cases'].iloc[i]\n",
    "        max_wave1_monthid_state_dict[state_wise_month_df['state'].iloc[i]]=state_wise_month_df['month'].iloc[i]\n",
    "        \n",
    "#wave2\n",
    "max_cases_state_dict=dict(zip(state_list,[0]*len(state_list)))\n",
    "max_wave2_monthid_state_dict=dict(zip(state_list,[1]*len(state_list)))\n",
    "for i in range(231,470):\n",
    "    if state_wise_month_df['cases'].iloc[i] > max_cases_state_dict[state_wise_month_df['state'].iloc[i]]:\n",
    "        max_cases_state_dict[state_wise_month_df['state'].iloc[i]]=state_wise_month_df['cases'].iloc[i]\n",
    "        max_wave2_monthid_state_dict[state_wise_month_df['state'].iloc[i]]=state_wise_month_df['month'].iloc[i]\n",
    "        \n",
    "\n",
    "#converting dictionaries to dataframe so that we can merge them at end easily\n",
    "test_df=pd.DataFrame.from_dict(max_wave1_monthid_state_dict.items())\n",
    "test7_df=pd.DataFrame()\n",
    "test7_df['state']=test_df[0]\n",
    "test7_df['wave1-monthid']=test_df[1]\n",
    "\n",
    "test_df=pd.DataFrame.from_dict(max_wave2_monthid_state_dict.items())\n",
    "test8_df=pd.DataFrame()\n",
    "test8_df['state']=test_df[0]\n",
    "test8_df['wave2-monthid']=test_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33d4cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_peak_df=pd.merge(test5_df,test6_df)\n",
    "month_peak_df=pd.merge(month_peak_df,test7_df)\n",
    "month_peak_df=pd.merge(month_peak_df,test8_df)\n",
    "\n",
    "'''Some states have all the entries as 1 of column wave1-weekid,  wave2-weekid, wave1-monthid and wave2-monthid\n",
    "This is supposedly due to reasons such as NEGATIVE ENTRIES in cowin-vaccine data and incorrect cummulative entries.\n",
    "These can be left as it is but I am going to remove them because they behave somewhat like outliers/noise'''\n",
    "#removing rows having all entries as 1\n",
    "month_peak_df=month_peak_df.drop(month_peak_df[(month_peak_df['wave1-weekid']==1)&(month_peak_df['wave2-weekid']==1)&(month_peak_df['wave1-monthid']==1)&(month_peak_df['wave2-monthid']==1) ].index,axis=0)\n",
    "#removing another outlier\n",
    "month_peak_df=month_peak_df.drop(month_peak_df[(month_peak_df['state']=='TG')].index,axis=0)\n",
    "\n",
    "\n",
    "month_peak_df.to_csv('output files/state-peaks.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d8ed0",
   "metadata": {},
   "source": [
    "## Overall-peaks.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3818d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_week_df=state_wise_df\n",
    "overall_week_df['state']=overall_week_df['state'].replace(to_replace='^.*',value='India',regex=True)\n",
    "overall_week_df=overall_week_df.rename(columns={'state':'overall'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2e390",
   "metadata": {},
   "source": [
    "### week wise peak calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92cae8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_week_df=overall_week_df.groupby(['overall','week']).sum()\n",
    "overall_week_df.to_csv('datasets/overall-week-df.csv')\n",
    "overall_week_df=pd.read_csv('datasets/overall-week-df.csv')\n",
    "\n",
    "wave1_weekid=1\n",
    "max_cases=0\n",
    "for i in range(60):\n",
    "    if overall_week_df['cases'].iloc[i] > max_cases:\n",
    "        max_cases=overall_week_df['cases'].iloc[i]\n",
    "        wave1_weekid=overall_week_df['week'].iloc[i]\n",
    "\n",
    "        \n",
    "wave2_weekid=61\n",
    "max_cases=0\n",
    "for i in range(61,135):\n",
    "    if overall_week_df['cases'].iloc[i] > max_cases:\n",
    "        max_cases=overall_week_df['cases'].iloc[i]\n",
    "        wave2_weekid=overall_week_df['week'].iloc[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054180d",
   "metadata": {},
   "source": [
    "### Month wise peak calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00b7a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_month_df=state_wise_month_df\n",
    "overall_month_df=overall_month_df.replace(to_replace='^.*',value='India',regex=True)\n",
    "overall_month_df.rename(columns={'state':'overall'},inplace=True)\n",
    "overall_month_df=overall_month_df.groupby(['overall','month']).sum()\n",
    "overall_month_df.to_csv('datasets/overall-month-df.csv',index=False) #groupby has been applied so the relation between columns has been changed.We need to save and load csv again\n",
    "overall_month_df=pd.read_csv('datasets/overall-month-df.csv')\n",
    "\n",
    "\n",
    "\n",
    "# There are only 15 rows. It is clear that wave1-month is 5th month(cases=2105105) and wave2-month is 13th month(cases=9109152)\n",
    "\n",
    "wave1_monthid=5\n",
    "wave2_monthid=13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e95b87",
   "metadata": {},
   "source": [
    "### creation of overall-peaks.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0ffe2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_peaks=pd.DataFrame(data={'overall':'India','wave1-weekid':wave1_weekid,'wave2-weekid':wave2_weekid,'wave1-monthid':5,'wave2_monthid':13},index=[0])\n",
    "overall_peaks.to_csv('output files/overall-peaks.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc000e78",
   "metadata": {},
   "source": [
    "#   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
